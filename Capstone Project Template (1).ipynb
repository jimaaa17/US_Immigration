{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# ETL Pipeline for Immigration and Temperature Data\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The goal of this project is to create an ETL pipeline using I94 immigration data and city temperature data to form a database that is optimized for queries on immigration events. This database can be used to answer questions relating immigration behavior to destination temperature e.g., do people tend to immigrate to warmer places?\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd, re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this project, we will aggregate I94 immigration data by destination city to form our first dimension table. Next we will aggregate city temperature data by city to form the second dimension table. The two datasets will be joined on destination city to form the fact table. The final database is optimized to query on immigration events to determine if temperature affects the selection of destination cities. Spark will be used to process the data.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "The I94 immigration [data](https://travel.trade.gov/research/reports/i94/historical/2016.html) comes from the US National Tourism and Trade Office. It is provided in SAS7BDAT [format](https://cran.r-project.org/web/packages/sas7bdat/vignettes/sas7bdat.pdf) which is a binary database storage format. Some relevant attributes include:\n",
    "\n",
    "* `i94yr` = 4 digit year\n",
    "* `i94mon` = numeric month\n",
    "* `i94cit` = 3 digit code of origin city\n",
    "* `i94port` = 3 character code of destination USA city\n",
    "* `arrdate` = arrival date in the USA\n",
    "* `i94mode` = 1 digit travel code\n",
    "* `depdate` = departure date from the USA\n",
    "* `i94visa` = reason for immigration\n",
    "\n",
    "The temperature [data](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data) comes from Kaggle. It is provided in csv format. Some relevant attributes include:\n",
    "\n",
    "* `AverageTemperature` = average temperature\n",
    "* `City` = city name\n",
    "* `Country` = country name\n",
    "* `Latitude`= latitude\n",
    "* `Longitude` = longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read June 2016 I94 immigration data into Pandas for exploration\n",
    "filename_1 = '../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat'\n",
    "df_sas = pd.read_sas(filename_1, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20612.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>10032016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.493846e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20612.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>10032016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.746006e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20609.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.679298e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20611.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.140963e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20632.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.934535e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    4.0  2016.0     6.0   135.0   135.0     XXX  20612.0      NaN     NaN   \n",
       "1    5.0  2016.0     6.0   135.0   135.0     XXX  20612.0      NaN     NaN   \n",
       "2    6.0  2016.0     6.0   213.0   213.0     XXX  20609.0      NaN     NaN   \n",
       "3    7.0  2016.0     6.0   213.0   213.0     XXX  20611.0      NaN     NaN   \n",
       "4   16.0  2016.0     6.0   245.0   245.0     XXX  20632.0      NaN     NaN   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto  gender  insnum  \\\n",
       "0      NaN   ...           U      NaN   1957.0  10032016     NaN     NaN   \n",
       "1      NaN   ...           U      NaN   1966.0  10032016     NaN     NaN   \n",
       "2      NaN   ...           U      NaN   1989.0       D/S     NaN     NaN   \n",
       "3      NaN   ...           U      NaN   1993.0       D/S     NaN     NaN   \n",
       "4      NaN   ...           U      NaN   1992.0       D/S     NaN     NaN   \n",
       "\n",
       "   airline        admnum  fltno visatype  \n",
       "0      NaN  1.493846e+10    NaN       WT  \n",
       "1      NaN  1.746006e+10    NaN       WT  \n",
       "2      NaN  1.679298e+09    NaN       F1  \n",
       "3      NaN  1.140963e+09    NaN       F1  \n",
       "4      NaN  1.934535e+09    NaN       F1  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Temperature data into Pandas for exploration\n",
    "filename_2= '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "\n",
    "df_temp = pd.read_csv(filename_2, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Ã…rhus   \n",
       "1  1743-12-01                 NaN                            NaN  Ã…rhus   \n",
       "2  1744-01-01                 NaN                            NaN  Ã…rhus   \n",
       "3  1744-02-01                 NaN                            NaN  Ã…rhus   \n",
       "4  1744-03-01                 NaN                            NaN  Ã…rhus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Spark session with SAS7BDAT jar\n",
    "spark = SparkSession\\\n",
    ".builder \\\n",
    ".config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore and clearning data Steps\n",
    "For the I94 immigration data, we will drop all entries where the destination city code i94port does not have A valid value (e.g., XXX, 99, etc) in I94_SAS_Labels_Description.SAS. For the temperature data, we will drop all entries where AverageTemperature is NaN, then drop all entries with duplicate locations, and then add the i94port of the location for each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "\n",
    "reg_obj = re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "i94port_valid = {}\n",
    "with open('i94port_valid.txt') as f:\n",
    "    for line in f:\n",
    "        match = reg_obj.search(line)\n",
    "        i94port_valid[match[1]]=[match[2]]\n",
    "        \n",
    "def clean_i94_data(file):\n",
    "    '''\n",
    "    Input: Path to I94 immigration data file\n",
    "    \n",
    "    Output: Spark dataframe of I94 immigration\n",
    "    data with valid i94port\n",
    "    '''\n",
    "    \n",
    "    #I94 data into spark\n",
    "    df_immigration = spark.read.format('com.github.saurfang.sas.spark').load(file)\n",
    "    \n",
    "    #Filter entries where i94port is not valid\n",
    "    df_immigration = df_immigration.filter(df_immigration.i94port.isin(list(i94port_valid.keys())))\n",
    "    return df_immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean temperature data\n",
    "df_temp=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../../data2/GlobalLandTemperaturesByCity.csv\")\n",
    "\n",
    "# Filter entries with NaN for average temperature\n",
    "df_temp=df_temp.filter(df_temp.AverageTemperature != 'NaN')\n",
    "\n",
    "# Remove duplicate locations\n",
    "df_temp=df_temp.dropDuplicates(['City', 'Country'])\n",
    "\n",
    "@udf()\n",
    "def get_i94port(city):\n",
    "    '''\n",
    "    Input: City name\n",
    "    \n",
    "    Output: Corresponding i94port\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    for key in i94port_valid:\n",
    "        if city.lower() in i94port_valid[key][0].lower():\n",
    "            return key\n",
    "\n",
    "# iport94 code based on city name\n",
    "df_temp=df_temp.withColumn(\"i94port\", get_i94port(df_temp.City))\n",
    "\n",
    "# Remove entries with 'null' iport94 code\n",
    "df_temp=df_temp.filter(df_temp.i94port != 'null')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The first dimension table will contain events from the I94 immigration data. The columns below will be extracted from the immigration dataframe:\n",
    "* `i94yr` = four digit year\n",
    "* `i94mon` =  numeric month\n",
    "* `i94cit` = three digit code of origin city\n",
    "* `i94port` =  three character code of destination city\n",
    "* `arrdate` = arrival date\n",
    "* `i94mode` = one digit travel code\n",
    "* `depdate` = departure date\n",
    "* `i94visa` = reason for immigration\n",
    "\n",
    "The second dimension table will contain city temperature data. The columns below will be extracted from the temperature dataframe:\n",
    "\n",
    "* `i94port` = 3 character code of destination city (mapped from immigration data during cleanup step)\n",
    "* `AverageTemperature` = average temperature\n",
    "* `City` = city name\n",
    "* `Country` = country name\n",
    "* `Latitude`= latitude\n",
    "* `Longitude` = longitude\n",
    "\n",
    "The fact table will contain information from the I94 immigration data joined with the city temperature data on `i94port`:\n",
    "* `i94yr` = four digit year\n",
    "* `i94mon` = numeric month\n",
    "* `i94cit` = three digit code of origin city\n",
    "* `i94port` = three character code of destination city\n",
    "* `arrdate` = arrival date\n",
    "* `i94mode` = one digit travel code\n",
    "* `depdate` = departure date\n",
    "* `i94visa` = reason for immigration\n",
    "* `AverageTemperature` = average temperature of destination city\n",
    "\n",
    "The tables will be saved to Parquet files partitioned by city (`i94port`). \n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The pipeline steps are described below:\n",
    "1. Clean I94 data as described in step 2 to create Spark dataframe `df_immigration` for each month\n",
    "2. Clean temperature data as described in step 2 to create Spark dataframe `df_temp` (already performed)\n",
    "3. Create immigration dimension table by selecting relevant columns from `df_immigration` and write to parquet file partitioned by `i94port`\n",
    "4. Create temperature dimension table by selecting relevant columns from `df_temp` and write to parquet file partitioned by `i94port`\n",
    "5. Create fact table by joining immigration and temperature dimension tables on `i94port` and write to parquet file partitioned by `i94port`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Path to I94 immigration data\n",
    "#immigration_data = '/data/18-83510-I94-Data-2016/*.sas7bdat'\n",
    "data_immigration = '/data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "\n",
    "#Clean I94 Immigration data and store as Spark Dataframe\n",
    "df_immigration = clean_i94_data(data_immigration)\n",
    "\n",
    "#Select columns for immigration dimension columns\n",
    "immigration_table = df_immigration.select([\"i94yr\", \"i94mon\", \"i94cit\", \"i94port\", \"arrdate\", \"i94mode\", \"depdate\", \"i94visa\"])\n",
    "\n",
    "#Write immigration dimension to parquet files partitioned by 'i94port'\n",
    "immigration_table.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"/results/immigration.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Select columns for temperature dimension table\n",
    "temp_table = df_temp.select([\"AverageTemperature\", \"City\", \"Country\",\"Latitude\", \"Longitude\", \"i94port\"])\n",
    "\n",
    "#Write immigration dimension table to parquet files partitioned by 'i94port'\n",
    "immigration_table.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"/results/immigration.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create temporary views of the immigraion and temperature data\n",
    "df_immigration.createOrReplaceTempView(\"immigration_view\")\n",
    "df_temp.createOrReplaceTempView(\"temp_view\")\n",
    "\n",
    "#Create the fact table by joining the immigration and temperature views\n",
    "fact_table = spark.sql('''\n",
    "SELECT immigration_view.i94yr as year,\n",
    "       immigration_view.i94mon as month,\n",
    "       immigration_view.i94cit as city,\n",
    "       immigration_view.i94port as i94port,\n",
    "       immigration_view.arrdate as arrival_date,\n",
    "       immigration_view.depdate as departure_date,\n",
    "       immigration_view.i94visa as reason,\n",
    "       temp_view.AverageTemperature as temperature,\n",
    "       temp_view.Latitude as latitude,\n",
    "       temp_view.Longitude as longitude\n",
    "FROM immigration_view\n",
    "INNER JOIN temp_view ON (immigration_view.i94port = temp_view.i94port)\n",
    "''')\n",
    "#Write fact table to parquet files partitioned by i94port\n",
    "fact_table.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"/results/fact.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data quality check passed for immigration table with 3088544 records\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "def quality_check(df, description):\n",
    "    '''\n",
    "    Input: spark dataframe, description of spark dataframe\n",
    "    \n",
    "    output: Print the outcome of data quality check\n",
    "    '''\n",
    "    record_count=df.count()\n",
    "    if record_count == 0:\n",
    "        print(\"Data quality check failed for {} with zero records\".format(description))\n",
    "    else:\n",
    "        print(\" Data quality check passed for {} with {} records\".format(description, record_count))\n",
    "            \n",
    "#Perform data quality check\n",
    "quality_check(df_immigration, \"immigration table\")\n",
    "quality_check(df_temp, \"temperature table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "The first dimension table will contain events from the I94 immigration data. The columns below will be extracted from the immigration dataframe:\n",
    "* `i94yr` =  four digit year\n",
    "* `i94mon` = numeric month\n",
    "* `i94cit` = three digit code of origin city\n",
    "* `i94port` = three character code of destination city\n",
    "* `arrdate` = arrival date\n",
    "* `i94mode` = digit travel code\n",
    "* `depdate` = departure date\n",
    "* `i94visa` = reason for immigration\n",
    "\n",
    "The second dimension table will contain city temperature data. The columns below will be extracted from the temperature dataframe:\n",
    "\n",
    "* `i94port` = character code of destination city (mapped from immigration data during cleanup step)\n",
    "* `AverageTemperature` = average temperature\n",
    "* `City` = city name\n",
    "* `Country` = country name\n",
    "* `Latitude`= latitude\n",
    "* `Longitude` = longitude\n",
    "\n",
    "The fact table will contain information from the I94 immigration data joined with the city temperature data on `i94port`:\n",
    "* `i94yr` = four digit year\n",
    "* `i94mon` = numeric month\n",
    "* `i94cit` = three digit code of origin city\n",
    "* `i94port` = three character code of destination city\n",
    "* `arrdate` = arrival date\n",
    "* `i94mode` = one digit travel code\n",
    "* `depdate` = departure date\n",
    "* `i94visa` = reason for immigration\n",
    "* `AverageTemperature` = average temperature of destination city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "Spark was chosen as it can easily handle multiple file formats (including SAS) containing large amounts of data. Spark SQL was chosen to process the large input files into dataframes and manipulated via standard SQL join operations to form additional tables.\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "\n",
    "The data should be updated monthly in coexistence with the current raw file format.\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " \n",
    " Our data would be stored in an Amazon S3 bucket and loaded to our staging tables. We would still use spark as it as our data processing platform since it is the best suited platform for very large datasets.\n",
    " \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " \n",
    " We would use Apache Airflow to perform the ETL and data qualtiy validation.\n",
    " \n",
    " * The database needed to be accessed by 100+ people.\n",
    " \n",
    " Once the data is ready to be accessed, it would be stored in a postgres database on a redshift cluster that easily supports multiuser access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
